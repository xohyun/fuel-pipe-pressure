{"cells":[{"cell_type":"markdown","source":["## Mount drive & import"],"metadata":{"id":"H3eN0oaUUbB-"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21757,"status":"ok","timestamp":1666224826264,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"},"user_tz":-540},"id":"XltI_t_kUGvD","outputId":"6b89d97a-a16d-476c-ace5-cf174d466dec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1j1N0u5t0l99N_wfSd5UZvnhugzn5g_NC/TimeSeriesAnomaly/data/modify\n"]}],"source":["from google.colab import drive\n","# drive.mount('/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data')\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/IITP/sohyun/TimeSeriesAnomaly/data/modify"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qi5fjXQLHQab","executionInfo":{"status":"ok","timestamp":1666224826265,"user_tz":-540,"elapsed":6,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":["# !pip install wandb -qqq\n","# import wandb\n","# wandb.login()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gq_QRBw-_yqo","executionInfo":{"status":"ok","timestamp":1666224829677,"user_tz":-540,"elapsed":3416,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import easydict\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","import random\n","import pandas as pd\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","import time\n","import math\n","import easydict\n","import seaborn as sns\n","from pylab import rcParams\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","import copy\n","from scipy import stats"]},{"cell_type":"markdown","metadata":{"id":"zjoJxI3iylhf"},"source":["## Data "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"yH_ixp0aWdm4","executionInfo":{"status":"ok","timestamp":1666224832492,"user_tz":-540,"elapsed":2827,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":["#---# Setting path #---#\n","save_path = \"/content/drive/MyDrive/IITP/sohyun/TimeSeriesAnomaly/data/modify\"\n","\n","#---# LOAD npy file #---#\n","Fu_20_normal = np.load('Fu_20_normal.npy')\n","Fu_21_normal = np.load('Fu_21_normal.npy')\n","Fu_21_abnormal = np.load('Fu_21_abnormal.npy')\n","Fu_22_normal = np.load('Fu_22_normal.npy')\n","Fu_22_abnormal = np.load('Fu_22_abnormal.npy')\n","\n","Fu_20_normal_10 = np.load('Fu_20_normal_10.npy')\n","Fu_21_normal_10 = np.load('Fu_21_normal_10.npy')\n","Fu_21_abnormal_10 = np.load('Fu_21_abnormal_10.npy')\n","Fu_22_normal_10 = np.load('Fu_22_normal_10.npy')\n","Fu_22_abnormal_10 = np.load('Fu_22_abnormal_10.npy')\n","\n","# import sys\n","# np.set_printoptions(threshold=sys.maxsize) # print all\n","\n","#---# 확인용 #---#\n","# plt.figure(figsize=(30,5))\n","# plt.plot(Fu_22_abnormal_10)"]},{"cell_type":"markdown","source":["Dataset"],"metadata":{"id":"7d_0GAXtSXqU"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"fS479XwTHmk0","executionInfo":{"status":"ok","timestamp":1666224832492,"user_tz":-540,"elapsed":6,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":["class MyDataset(Dataset):\n","  def __init__(self, config, mode=\"train\"):\n","    # 최종 목표 : path\n","    # self.data_path_list = data_path_list\n","    # self.data_list = [_load_data() for data_path in self.data_path_list] \n","    self.mode = mode\n","    self.config = config\n","    self.device = self.config.device\n","    self.train_list = [Fu_22_normal_10, Fu_21_normal_10, Fu_20_normal_10]\n","    self.test_list = [Fu_22_abnormal_10]\n","    \n","    #---# scaling #---#\n","    self.scaled_train_list, self.scaled_test = self._scale()\n","    \n","    #---# sliding window & concat #---#\n","    self.slided_train_list = [self._sliding_window(data) for data in self.scaled_train_list]\n","    self.train = np.concatenate(self.slided_train_list, axis=0)\n","    \n","    #---# split #---#\n","    y = [0 for _ in range(len(self.train))]\n","    self.x_train, self.x_val, _, _ = train_test_split(self.train, y, test_size=0.2, random_state=self.config.seed)\n","    self.x_test = self._sliding_window(self.scaled_test)\n","    \n","  def __getitem__(self, index):\n","    if self.mode == \"train\":\n","      self.x = self.x_train[index]\n","    elif self.mode == \"test\":\n","      self.x = self.x_test[index]\n","    elif self.mode == \"val\":\n","      self.x = self.x_val[index]\n","    return torch.tensor(self.x, dtype=torch.float32)\n","      \n","  def __len__(self):\n","    if self.mode == \"train\":\n","      self.x = self.x_train\n","    elif self.mode == \"test\":\n","      self.x = self.x_test\n","    elif self.mode == \"val\":\n","      self.x = self.x_val\n","    return len(self.x)\n","      \n","  def _scale(self):\n","    '''\n","    input : raw data\n","    output : scaled data\n","    '''\n","    train = np.concatenate(self.train_list, axis=0)\n","    test = self.test_list[0]\n","    total = np.concatenate([train, test])\n","\n","    scaler = StandardScaler()\n","    total_scaled = scaler.fit_transform(total.reshape(-1, 1)).squeeze()\n","    train_scaled = scaler.transform(train.reshape(-1,1)).squeeze()\n","    test_scaled = scaler.transform(test.reshape(-1,1)).squeeze()\n","\n","    train_list = []\n","    split_idx1, split_idx2 = len(self.train_list[0]), len(self.train_list[1]) \n","\n","    train_list.append(train_scaled[:split_idx1])\n","    train_list.append(train_scaled[split_idx1:split_idx1+split_idx2])\n","    train_list.append(train_scaled[split_idx1+split_idx2:])\n","\n","    return train_list, test_scaled\n","\n","  def _sliding_window(self, arr):\n","    start_pt = 0\n","    total_data = []\n","    \n","    while(True):\n","      if len(arr) < (start_pt + config.window_size) : break\n","      data = arr[start_pt:start_pt + config.window_size]\n","      start_pt += config.stride\n","      total_data.append(data)\n","\n","    return np.array(total_data)"]},{"cell_type":"markdown","source":["## Functions"],"metadata":{"id":"pFXdkFGgSg5M"}},{"cell_type":"code","source":["from pandas.core.arrays.sparse import array\n","def seed_everything(seed: int = 42):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed(seed)  # type: ignore\n","  torch.backends.cudnn.deterministic = True  # type: ignore\n","  torch.backends.cudnn.benchmark = True  # type: ignore\n","\n","def get_anomaly_time(original, prediction) : \n","  temp = np.zeros(shape=(len(original),), dtype=np.float32)\n","  # original = original.squeeze(axis = 1)\n","\n","  for i in range(len(prediction)) :\n","    if prediction[i] == 0 :\n","      temp[i*config.stride : (i*config.stride + config.window_size)] = np.nan\n","\n","    elif prediction[i] == 1 : # anomaly\n","      temp[i*config.stride : (i*config.stride + config.window_size)] = original[i*config.stride : (i*config.stride + config.window_size)]\n","\n","  return temp\n","\n","def drawing(pred, x) :\n","  #---# Drawing - 22 #---#\n","  plt.figure(figsize=(30,5))\n","  plt.plot(x, markersize=1)\n","  plt.plot(pred, marker='.', markersize=2, color='r', linestyle='None')\n","\n","  #---# 실제 anomaly 값 구간 #---#\n","  a = np.linspace(62200, 65300)\n","  # plt.fill_between(a, 0, 2000, color='green', alpha=0.3)\n","  plt.fill_between(a, -1, 4, color='green', alpha=0.5)\n","  b = np.linspace(95600, 99300)\n","  # plt.fill_between(b, 0, 2000, color='green', alpha=0.5)\n","  plt.fill_between(b, -1, 4, color='green', alpha=0.5)\n","  c = np.linspace(148400, 152300)\n","  # plt.fill_between(c, 0, 2000, color='green', alpha=0.5)\n","  plt.fill_between(c, -1, 4, color='green', alpha=0.5)\n","\n","  plt.show()\n","  plt.clf()\n","\n","def calculate(true_list, pred_list): \n","  pred_list = pred_list.dropna()\n","\n","  pred_anomaly_set = set(pred_list.index.tolist())\n","  pred_normal_set = set(range(len(true_list))) - pred_anomaly_set\n","  true_anomaly_set = set(np.where(np.array(true_list) != 0)[0].tolist())\n","  true_normal_set = set(np.where(np.array(true_list) == 0)[0].tolist())\n","\n","  recall = len(pred_anomaly_set.intersection(true_anomaly_set)) / len(true_anomaly_set)\n","  precision = len(pred_anomaly_set.intersection(true_anomaly_set)) / len(pred_anomaly_set.union(true_anomaly_set)) # len(anomaly_set.union(true_set))\n","  accuracy = (len(pred_anomaly_set.intersection(true_anomaly_set)) + len(pred_normal_set.intersection(true_normal_set))) / len(true_list) # (빨간 거 맞은거 + 파란거 맞은거) / 전체\n","  return recall, precision, accuracy\n","\n","def get_true_label(test_dataset):\n","  ##########################\n","  #---# true list 만들기 #---#\n","  ##########################\n","  true_label = [0 for i in range(len(test_dataset.scaled_test))] # test_scale\n","  true_label[62200:65300] = [1 for i in range(62200,65300)]\n","  true_label[95600:99300] = [1 for i in range(95600,99300)]\n","  true_label[148400:152300] = [1 for i in range(148400,152300)]\n","\n","  # true_label_sliding = sliding_window(np.array(true_label), config.window_size, config.stride)\n","  # true_label_sliding = np.expand_dims(np.array(true_label_sliding), 2) # dimension expansion\n","  return true_label\n"],"metadata":{"id":"GD_NyF6OSftq","executionInfo":{"status":"ok","timestamp":1666224832960,"user_tz":-540,"elapsed":472,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Reconstruction error"],"metadata":{"id":"R29OhR0YSTh0"}},{"cell_type":"code","source":["\"\"\"\n","Time Series error calculation functions.\n","\"\"\"\n","\n","import math\n","\n","import numpy as np\n","import pandas as pd\n","# from pyts.metrics import dtw\n","from scipy import integrate\n","\n","\n","def regression_errors(y, y_hat, smoothing_window=0.01, smooth=True):\n","  \"\"\"Compute an array of absolute errors comparing predictions and expected output.\n","  If smooth is True, apply EWMA to the resulting array of errors.\n","  Args:\n","    y (ndarray):\n","      Ground truth.\n","    y_hat (ndarray):\n","      Predicted values.\n","    smoothing_window (float):\n","      Optional. Size of the smoothing window, expressed as a proportion of the total\n","      length of y. If not given, 0.01 is used.\n","    smooth (bool):\n","      Optional. Indicates whether the returned errors should be smoothed with EWMA.\n","      If not given, `True` is used.\n","  Returns:\n","    ndarray:\n","      Array of errors.\n","  \"\"\"\n","  errors = np.abs(y - y_hat)[:, 0]\n","\n","  if not smooth:\n","    return errors\n","\n","  smoothing_window = int(smoothing_window * len(y))\n","\n","  return pd.Series(errors).ewm(span=smoothing_window).mean().values\n","\n","\n","def _point_wise_error(y, y_hat):\n","  \"\"\"Compute point-wise error between predicted and expected values.\n","  The computed error is calculated as the difference between predicted\n","  and expected values with a rolling smoothing factor.\n","  Args:\n","    y (ndarray):\n","      Ground truth.\n","    y_hat (ndarray):\n","      Predicted values.\n","  Returns:\n","    ndarray:\n","      An array of smoothed point-wise error.\n","  \"\"\"\n","  return abs(y - y_hat)\n","\n","\n","def _area_error(y, y_hat, score_window=10):\n","  \"\"\"Compute area error between predicted and expected values.\n","  The computed error is calculated as the area difference between predicted\n","  and expected values with a smoothing factor.\n","  Args:\n","    y (ndarray):\n","      Ground truth.\n","    y_hat (ndarray):\n","      Predicted values.\n","    score_window (int):\n","      Optional. Size of the window over which the scores are calculated.\n","      If not given, 10 is used.\n","  Returns:\n","    ndarray:\n","      An array of area error.\n","  \"\"\"\n","  smooth_y = pd.Series(y).rolling(\n","      score_window, center=True, min_periods=score_window // 2).apply(integrate.trapz)\n","  smooth_y_hat = pd.Series(y_hat).rolling(\n","      score_window, center=True, min_periods=score_window // 2).apply(integrate.trapz)\n","\n","  errors = abs(smooth_y - smooth_y_hat)\n","\n","  return errors\n","\n","\n","def _dtw_error(y, y_hat, score_window=10):\n","  \"\"\"Compute dtw error between predicted and expected values.\n","  The computed error is calculated as the dynamic time warping distance\n","  between predicted and expected values with a smoothing factor.\n","  Args:\n","    y (ndarray):\n","      Ground truth.\n","    y_hat (ndarray):\n","      Predicted values.\n","    score_window (int):\n","      Optional. Size of the window over which the scores are calculated.\n","      If not given, 10 is used.\n","  Returns:\n","    ndarray:\n","      An array of dtw error.\n","  \"\"\"\n","  length_dtw = (score_window // 2) * 2 + 1\n","  half_length_dtw = length_dtw // 2\n","\n","  # add padding\n","  y_pad = np.pad(y, (half_length_dtw, half_length_dtw),\n","                  'constant', constant_values=(0, 0))\n","  y_hat_pad = np.pad(y_hat, (half_length_dtw, half_length_dtw),\n","                  'constant', constant_values=(0, 0))\n","\n","  i = 0\n","  similarity_dtw = list()\n","  while i < len(y) - length_dtw:\n","    true_data = y_pad[i:i + length_dtw]\n","    true_data = true_data.flatten()\n","\n","    pred_data = y_hat_pad[i:i + length_dtw]\n","    pred_data = pred_data.flatten()\n","\n","    dist = dtw(true_data, pred_data)\n","    similarity_dtw.append(dist)\n","    i += 1\n","\n","  errors = ([0] * half_length_dtw + similarity_dtw +\n","            [0] * (len(y) - len(similarity_dtw) - half_length_dtw))\n","\n","  return errors\n","\n","\n","def reconstruction_errors(y, y_hat, step_size=1, score_window=10, smoothing_window=0.01,\n","                        smooth=True, rec_error_type='point'):\n","  \"\"\"Compute an array of reconstruction errors.\n","  Compute the discrepancies between the expected and the\n","  predicted values according to the reconstruction error type.\n","  Args:\n","    y (ndarray):\n","      Ground truth.\n","    y_hat (ndarray):\n","      Predicted values. Each timestamp has multiple predictions.\n","    step_size (int):\n","      Optional. Indicating the number of steps between windows in the predicted values.\n","      If not given, 1 is used.\n","    score_window (int):\n","      Optional. Size of the window over which the scores are calculated.\n","      If not given, 10 is used.\n","    smoothing_window (float or int):\n","      Optional. Size of the smoothing window, when float it is expressed as a proportion\n","      of the total length of y. If not given, 0.01 is used.\n","    smooth (bool):\n","      Optional. Indicates whether the returned errors should be smoothed.\n","      If not given, `True` is used.\n","    rec_error_type (str):\n","      Optional. Reconstruction error types ``[\"point\", \"area\", \"dtw\"]``.\n","      If not given, \"point\" is used.\n","  Returns:\n","    ndarray:\n","      Array of reconstruction errors.\n","  \"\"\"\n","  y = y.reshape(y.shape[0], -1, 1) # 추가 부분 y.shape = (932, 1000, 1)\n","  y_hat = y_hat.reshape(y_hat.shape[0], -1, 1) # 추가 부분 y_hat shape = (932, 1000, 1)\n","  \n","  if isinstance(smoothing_window, float):\n","    smoothing_window = min(math.trunc(len(y) * smoothing_window), 200) # 9\n","    # len(y) * smoothing_window = 9 (932 * 0.01)\n","\n","  true = [item[0] for item in y.reshape((y.shape[0], -1))] # 932개\n","\n","  for item in y[-1][1:]: # y[-1][1] shape : (999,1)\n","    true.extend(item) # 총 개수 1931\n","  \n","  predictions = []\n","  predictions_vs = []\n","\n","  pred_length = y_hat.shape[1] # 1000 \n","  num_errors = y_hat.shape[1] + step_size * (y_hat.shape[0] - 1) # 1931\n","\n","  for i in range(num_errors): \n","    intermediate = []\n","    for j in range(max(0, i - num_errors + pred_length), min(i + 1, pred_length)):\n","      intermediate.append(y_hat[i - j, j])\n","\n","    if intermediate:\n","      predictions.append(np.median(np.asarray(intermediate)))\n","\n","      predictions_vs.append([[\n","          np.min(np.asarray(intermediate)),\n","          np.percentile(np.asarray(intermediate), 25),\n","          np.percentile(np.asarray(intermediate), 50),\n","          np.percentile(np.asarray(intermediate), 75),\n","          np.max(np.asarray(intermediate))\n","      ]])\n","\n","  true = np.asarray(true)\n","  predictions = np.asarray(predictions)\n","  predictions_vs = np.asarray(predictions_vs)\n","\n","  # Compute reconstruction errors\n","  if rec_error_type.lower() == \"point\":\n","    errors = _point_wise_error(true, predictions)\n","\n","  elif rec_error_type.lower() == \"area\":\n","    errors = _area_error(true, predictions, score_window)\n","\n","  elif rec_error_type.lower() == \"dtw\":\n","    errors = _dtw_error(true, predictions, score_window)\n","\n","  # Apply smoothing\n","  if smooth:\n","    errors = pd.Series(errors).rolling(\n","      smoothing_window, center=True, min_periods=smoothing_window // 2).mean().values\n","\n","  return errors, predictions_vs"],"metadata":{"id":"yBSxIvekSI2w","executionInfo":{"status":"ok","timestamp":1666224832960,"user_tz":-540,"elapsed":5,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Find anomaly"],"metadata":{"id":"b2Vy_C9kSEO_"}},{"cell_type":"code","source":["\"\"\"\n","Time Series anomaly detection functions.\n","Some of the implementation is inspired by the paper https://arxiv.org/pdf/1802.04431.pdf\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.optimize import fmin\n","\n","\n","def deltas(errors, epsilon, mean, std):\n","  \"\"\"Compute mean and std deltas.\n","  delta_mean = mean(errors) - mean(all errors below epsilon)\n","  delta_std = std(errors) - std(all errors below epsilon)\n","  Args:\n","    errors (ndarray):\n","      Array of errors.\n","    epsilon (ndarray):\n","      Threshold value.\n","    mean (float):\n","      Mean of errors.\n","    std (float):\n","      Standard deviation of errors.\n","  Returns:\n","    float, float:\n","      * delta_mean.\n","      * delta_std.\n","  \"\"\"\n","  below = errors[errors <= epsilon]\n","  if not len(below):\n","    return 0, 0\n","\n","  return mean - below.mean(), std - below.std()\n","\n","\n","def count_above(errors, epsilon):\n","  \"\"\"Count number of errors and continuous sequences above epsilon.\n","  Continuous sequences are counted by shifting and counting the number\n","  of positions where there was a change and the original value was true,\n","  which means that a sequence started at that position.\n","  Args:\n","    errors (ndarray):\n","      Array of errors.\n","    epsilon (ndarray):\n","      Threshold value.\n","  Returns:\n","    int, int:\n","      * Number of errors above epsilon.\n","      * Number of continuous sequences above epsilon.\n","  \"\"\"\n","  above = errors > epsilon\n","  total_above = len(errors[above])\n","\n","  above = pd.Series(above)\n","  shift = above.shift(1)\n","  change = above != shift\n","\n","  total_consecutive = sum(above & change)\n","\n","  return total_above, total_consecutive\n","\n","\n","def z_cost(z, errors, mean, std):\n","  \"\"\"Compute how bad a z value is.\n","  The original formula is::\n","                (delta_mean/mean) + (delta_std/std)\n","      ------------------------------------------------------\n","      number of errors above + (number of sequences above)^2\n","  which computes the \"goodness\" of `z`, meaning that the higher the value\n","  the better the `z`.\n","  In this case, we return this value inverted (we make it negative), to convert\n","  it into a cost function, as later on we will use scipy.fmin to minimize it.\n","  Args:\n","    z (ndarray):\n","      Value for which a cost score is calculated.\n","    errors (ndarray):\n","      Array of errors.\n","    mean (float):\n","      Mean of errors.\n","    std (float):\n","      Standard deviation of errors.\n","  Returns:\n","    float:\n","      Cost of z.\n","  \"\"\"\n","  epsilon = mean + z * std\n","\n","  delta_mean, delta_std = deltas(errors, epsilon, mean, std)\n","  above, consecutive = count_above(errors, epsilon)\n","\n","  numerator = -(delta_mean / mean + delta_std / std)\n","  denominator = above + consecutive ** 2\n","\n","  if denominator == 0:\n","    return np.inf\n","\n","  return numerator / denominator\n","\n","\n","def _find_threshold(errors, z_range):\n","  \"\"\"Find the ideal threshold.\n","  The ideal threshold is the one that minimizes the z_cost function. Scipy.fmin is used\n","  to find the minimum, using the values from z_range as starting points.\n","  Args:\n","    errors (ndarray):\n","      Array of errors.\n","    z_range (list):\n","      List of two values denoting the range out of which the start points for the\n","      scipy.fmin function are chosen.\n","  Returns:\n","    float:\n","      Calculated threshold value.\n","  \"\"\"\n","  mean = errors.mean()\n","  std = errors.std()\n","\n","  min_z, max_z = z_range\n","  best_z = min_z\n","  best_cost = np.inf\n","  for z in range(min_z, max_z):\n","    best = fmin(z_cost, z, args=(errors, mean, std), full_output=True, disp=False) # minimize\n","    z, cost = best[0:2]\n"," \n","    if cost < best_cost:\n","      best_z = z[0]\n","\n","  return mean + best_z * std\n","\n","\n","def _fixed_threshold(errors, k=4):\n","  \"\"\"Calculate the threshold.\n","  The fixed threshold is defined as k standard deviations away from the mean.\n","  Args:\n","    errors (ndarray):\n","      Array of errors.\n","  Returns:\n","    float:\n","      Calculated threshold value.\n","  \"\"\"\n","  mean = errors.mean()\n","  std = errors.std()\n","\n","  return mean + k * std\n","\n","\n","def _find_sequences(errors, epsilon, anomaly_padding):\n","  \"\"\"Find sequences of values that are above epsilon.\n","  This is done following this steps:\n","    * create a boolean mask that indicates which values are above epsilon.\n","    * mark certain range of errors around True values with a True as well.\n","    * shift this mask by one place, filing the empty gap with a False.\n","    * compare the shifted mask with the original one to see if there are changes.\n","    * Consider a sequence start any point which was true and has changed.\n","    * Consider a sequence end any point which was false and has changed.\n","  Args:\n","    errors (ndarray):\n","      Array of errors.\n","    epsilon (float):\n","      Threshold value. All errors above epsilon are considered an anomaly.\n","    anomaly_padding (int):\n","      Number of errors before and after a found anomaly that are added to the\n","      anomalous sequence.\n","  Returns:\n","    ndarray, float:\n","      * Array containing start, end of each found anomalous sequence.\n","      * Maximum error value that was not considered an anomaly.\n","  \"\"\"\n","  above = pd.Series(errors > epsilon)\n","  index_above = np.argwhere(above.values)\n","\n","  for idx in index_above.flatten():\n","    above[max(0, idx - anomaly_padding):min(idx + anomaly_padding + 1, len(above))] = True\n","\n","  shift = above.shift(1).fillna(False)\n","  change = above != shift\n","\n","  if above.all():\n","    max_below = 0\n","  else:\n","    max_below = max(errors[~above])\n","\n","  index = above.index\n","  starts = index[above & change].tolist()\n","  ends = (index[~above & change] - 1).tolist()\n","\n","  if len(ends) == len(starts) - 1:\n","    ends.append(len(above) - 1)\n","\n","  return np.array([starts, ends]).T, max_below\n","\n","\n","def _get_max_errors(errors, sequences, max_below):\n","  \"\"\"Get the maximum error for each anomalous sequence.\n","  Also add a row with the max error which was not considered anomalous.\n","  Table containing a ``max_error`` column with the maximum error of each\n","  sequence and the columns ``start`` and ``stop`` with the corresponding start and stop\n","  indexes, sorted descendingly by the maximum error.\n","  Args:\n","    errors (ndarray):\n","      Array of errors.\n","    sequences (ndarray):\n","      Array containing start, end of anomalous sequences\n","    max_below (float):\n","      Maximum error value that was not considered an anomaly.\n","  Returns:\n","    pandas.DataFrame:\n","      DataFrame object containing columns ``start``, ``stop`` and ``max_error``.\n","  \"\"\"\n","  max_errors = [{\n","    'max_error': max_below,\n","    'start': -1,\n","    'stop': -1\n","  }]\n","\n","  for sequence in sequences:\n","    start, stop = sequence\n","    sequence_errors = errors[start: stop + 1]\n","    max_errors.append({\n","        'start': start,\n","        'stop': stop,\n","        'max_error': max(sequence_errors)\n","    })\n","\n","  max_errors = pd.DataFrame(max_errors).sort_values('max_error', ascending=False)\n","  return max_errors.reset_index(drop=True)\n","\n","\n","def _prune_anomalies(max_errors, min_percent):\n","  \"\"\"Prune anomalies to mitigate false positives.\n","  This is done by following these steps:\n","    * Shift the errors 1 negative step to compare each value with the next one.\n","    * Drop the last row, which we do not want to compare.\n","    * Calculate the percentage increase for each row.\n","    * Find rows which are below ``min_percent``.\n","    * Find the index of the latest of such rows.\n","    * Get the values of all the sequences above that index.\n","  Args:\n","    max_errors (pandas.DataFrame):\n","      DataFrame object containing columns ``start``, ``stop`` and ``max_error``.\n","    min_percent (float):\n","      Percentage of separation the anomalies need to meet between themselves and the\n","      highest non-anomalous error in the window sequence.\n","  Returns:\n","    ndarray:\n","      Array containing start, end, max_error of the pruned anomalies.\n","  \"\"\"\n","  next_error = max_errors['max_error'].shift(-1).iloc[:-1]\n","  max_error = max_errors['max_error'].iloc[:-1]\n","\n","  increase = (max_error - next_error) / max_error\n","  too_small = increase < min_percent\n","\n","  if too_small.all():\n","    last_index = -1\n","  else:\n","    last_index = max_error[~too_small].index[-1]\n","\n","  return max_errors[['start', 'stop', 'max_error']].iloc[0: last_index + 1].values\n","\n","\n","def _compute_scores(pruned_anomalies, errors, threshold, window_start):\n","  \"\"\"Compute the score of the anomalies.\n","  Calculate the score of the anomalies proportional to the maximum error in the sequence\n","  and add window_start timestamp to make the index absolute.\n","  Args:\n","    pruned_anomalies (ndarray):\n","      Array of anomalies containing the start, end and max_error for all anomalies in\n","        the window.\n","      errors (ndarray):\n","        Array of errors.\n","      threshold (float):\n","        Threshold value.\n","      window_start (int):\n","        Index of the first error value in the window.\n","  Returns:\n","    list:\n","      List of anomalies containing start-index, end-index, score for each anomaly.\n","  \"\"\"\n","  anomalies = list()\n","  denominator = errors.mean() + errors.std()\n","\n","  for row in pruned_anomalies:\n","    max_error = row[2]\n","    score = (max_error - threshold) / denominator\n","    anomalies.append([row[0] + window_start, row[1] + window_start, score])\n","\n","  return anomalies\n","\n","\n","def _merge_sequences(sequences):\n","  \"\"\"Merge consecutive and overlapping sequences.\n","  We iterate over a list of start, end, score triples and merge together\n","  overlapping or consecutive sequences.\n","  The score of a merged sequence is the average of the single scores,\n","  weighted by the length of the corresponding sequences.\n","  Args:\n","    sequences (list):\n","      List of anomalies, containing start-index, end-index, score for each anomaly.\n","  Returns:\n","    ndarray:\n","      Array containing start-index, end-index, score for each anomaly after merging.\n","  \"\"\"\n","  if len(sequences) == 0:\n","    return np.array([])\n","\n","  sorted_sequences = sorted(sequences, key=lambda entry: entry[0])\n","  new_sequences = [sorted_sequences[0]]\n","  score = [sorted_sequences[0][2]]\n","  weights = [sorted_sequences[0][1] - sorted_sequences[0][0]]\n","\n","  for sequence in sorted_sequences[1:]:\n","    prev_sequence = new_sequences[-1]\n","\n","    if sequence[0] <= prev_sequence[1] + 1:\n","      score.append(sequence[2])\n","      weights.append(sequence[1] - sequence[0])\n","      weighted_average = np.average(score, weights=weights)\n","      new_sequences[-1] = (prev_sequence[0], max(prev_sequence[1], sequence[1]),\n","                            weighted_average)\n","    else:\n","      score = [sequence[2]]\n","      weights = [sequence[1] - sequence[0]]\n","      new_sequences.append(sequence)\n","\n","  return np.array(new_sequences)\n","\n","\n","def _find_window_sequences(window, z_range, anomaly_padding, min_percent, window_start,\n","                           fixed_threshold):\n","  \"\"\"Find sequences of values that are anomalous.\n","  We first find the threshold for the window, then find all sequences above that threshold.\n","  After that, we get the max errors of the sequences and prune the anomalies. Lastly, the\n","  score of the anomalies is computed.\n","  Args:\n","    window (ndarray):\n","      Array of errors in the window that is analyzed.\n","    z_range (list):\n","      List of two values denoting the range out of which the start points for the\n","      dynamic find_threshold function are chosen.\n","    anomaly_padding (int):\n","      Number of errors before and after a found anomaly that are added to the anomalous\n","      sequence.\n","    min_percent (float):\n","      Percentage of separation the anomalies need to meet between themselves and the\n","      highest non-anomalous error in the window sequence.\n","    window_start (int):\n","      Index of the first error value in the window.\n","    fixed_threshold (bool):\n","      Indicates whether to use fixed threshold or dynamic threshold.\n","  Returns:\n","    ndarray:\n","      Array containing the start-index, end-index, score for each anomalous sequence\n","      that was found in the window.\n","  \"\"\"\n","  if fixed_threshold:\n","    threshold = _fixed_threshold(window)\n","\n","  else:\n","    threshold = _find_threshold(window, z_range)\n","\n","  window_sequences, max_below = _find_sequences(window, threshold, anomaly_padding)\n","  max_errors = _get_max_errors(window, window_sequences, max_below)\n","  pruned_anomalies = _prune_anomalies(max_errors, min_percent)\n","  window_sequences = _compute_scores(pruned_anomalies, window, threshold, window_start)\n","\n","  return window_sequences\n","\n","\n","def find_anomalies(errors, index, z_range=(0, 10), window_size=None, window_size_portion=None,\n","                   window_step_size=None, window_step_size_portion=None, min_percent=0.1,\n","                   anomaly_padding=50, lower_threshold=False, fixed_threshold=None):\n","  \"\"\"Find sequences of error values that are anomalous.\n","  We first define the window of errors, that we want to analyze. We then find the anomalous\n","  sequences in that window and store the start/stop index pairs that correspond to each\n","  sequence, along with its score. Optionally, we can flip the error sequence around the mean\n","  and apply the same procedure, allowing us to find unusually low error sequences.\n","  We then move the window and repeat the procedure.\n","  Lastly, we combine overlapping or consecutive sequences.\n","  Args:\n","    errors (ndarray):\n","      Array of errors.\n","    index (ndarray):\n","      Array of indices of the errors.\n","    z_range (list):\n","      Optional. List of two values denoting the range out of which the start points for\n","      the scipy.fmin function are chosen. If not given, (0, 10) is used.\n","    window_size (int):\n","      Optional. Size of the window for which a threshold is calculated. If not given,\n","      `None` is used, which finds one threshold for the entire sequence of errors.\n","    window_size_portion (float):\n","      Optional. Specify the size of the window to be a portion of the sequence of errors.\n","      If not given, `None` is used, and window size is used as is.\n","    window_step_size (int):\n","      Optional. Number of steps the window is moved before another threshold is\n","      calculated for the new window.\n","    window_step_size_portion (float):\n","      Optional. Specify the number of steps to be a portion of the window size. If not given,\n","      `None` is used, and window step size is used as is.\n","    min_percent (float):\n","      Optional. Percentage of separation the anomalies need to meet between themselves and\n","      the highest non-anomalous error in the window sequence. It nof given, 0.1 is used.\n","    anomaly_padding (int):\n","      Optional. Number of errors before and after a found anomaly that are added to the\n","      anomalous sequence. If not given, 50 is used.\n","    lower_threshold (bool):\n","      Optional. Indicates whether to apply a lower threshold to find unusually low errors.\n","      If not given, `False` is used.\n","    fixed_threshold (bool):\n","      Optional. Indicates whether to use fixed threshold or dynamic threshold. If not\n","      given, `False` is used.\n","  Returns:\n","    ndarray:\n","      Array containing start-index, end-index, score for each anomalous sequence that\n","      was found.\n","  \"\"\"\n","  window_size = window_size or len(errors)\n","  if window_size_portion:\n","    window_size = np.ceil(len(errors) * window_size_portion).astype('int')\n","\n","  window_step_size = window_step_size or window_size\n","  if window_step_size_portion:\n","    window_step_size = np.ceil(window_size * window_step_size_portion).astype('int')\n","\n","  window_start = 0\n","  window_end = 0\n","  sequences = list()\n","\n","  while window_end < len(errors):\n","    window_end = window_start + window_size\n","    window = errors[window_start:window_end]\n","    window_sequences = _find_window_sequences(window, z_range, anomaly_padding, min_percent,\n","                                              window_start, fixed_threshold)\n","    sequences.extend(window_sequences)\n","\n","    if lower_threshold:\n","      # Flip errors sequence around mean\n","      mean = window.mean()\n","      inverted_window = mean - (window - mean)\n","      inverted_window_sequences = _find_window_sequences(inverted_window, z_range,\n","                                                          anomaly_padding, min_percent,\n","                                                          window_start, fixed_threshold)\n","      sequences.extend(inverted_window_sequences)\n","\n","    window_start = window_start + window_step_size\n","\n","  sequences = _merge_sequences(sequences)\n","\n","  anomalies = list()\n","\n","  for start, stop, score in sequences:\n","    anomalies.append([index[int(start)], index[int(stop)], score])\n","\n","  return np.asarray(anomalies)"],"metadata":{"id":"rYb6rsuUSD0A","executionInfo":{"status":"ok","timestamp":1666224832961,"user_tz":-540,"elapsed":6,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from scipy.integrate._ivp.radau import P\n","def ad_test(test_loader, encoder, decoder, critic_x):\n","    reconstruction_error = list()\n","    critic_score = list()\n","    y_true = list()\n","    # preds=[] ##\n","    for batch, sample in enumerate(test_loader): # sample shape : [64, 1000]\n","        # reconstructed_signal = decoder(encoder(sample['signal']))\n","        reconstructed_signal = decoder(encoder(sample)) # [1,64,1000]\n","        reconstructed_signal = torch.squeeze(reconstructed_signal) # [64, 1000]\n","\n","        # preds.append(reconstructed_signal) ##\n","\n","        for i in range(0, 64):\n","            x_ = reconstructed_signal[i].detach().numpy() # [1000, ]\n","            x = sample[i].numpy() # [1000,]\n","            # y_true.append(int(sample['anomaly'][i].detach()))\n","            # y_true.append(int(sample[i].detach()))######\n","            # reconstruction_error.append(dtw_reconstruction_error(x, x_))\n","            cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n","\n","            cosine_sim = cos(torch.tensor(x_.reshape(-1,1)), torch.tensor(x.reshape(-1,1))) ##\n","            reconstruction_error.append(cosine_sim[0].cpu().detach().numpy()) ##\n","\n","        # critic_score.extend(torch.squeeze(critic_x(sample['signal'])).detach().numpy())\n","        critic_score.extend(torch.squeeze(critic_x(sample)).detach().numpy()) # [1,64,1]\n","\n","    # reconstruction_error = stats.zscore(reconstruction_error)\n","\n","    # predictions = torch.cat(preds) ## \n","    # errors, predictions_vs = reconstruction_errors(test_dataset.x_test[:len(predictions)], predictions.cpu().detach().numpy(), score_window=config.window_size, step_size=1)\n","\n","    critic_score = stats.zscore(critic_score)\n","    anomaly_score = reconstruction_error * critic_score\n","    y_predict = detect_anomaly(anomaly_score)\n","    y_predict = prune_false_positive(y_predict, anomaly_score, change_threshold=0.1)\n","    print(y_predict.shape) # 896,\n","\n","    y_true = true_labeling[:len(y_predict)] ##\n","    find_scores(y_true, y_predict)\n","\n","#Other error metrics - point wise difference, Area difference.\n","def dtw_reconstruction_error(x, x_):\n","    n, m = x.shape[0], x_.shape[0]\n","    dtw_matrix = np.zeros((n+1, m+1))\n","    for i in range(n+1):\n","        for j in range(m+1):\n","            dtw_matrix[i, j] = np.inf\n","    dtw_matrix[0, 0] = 0\n","\n","    for i in range(1, n+1):\n","        for j in range(1, m+1):\n","            cost = abs(x[i-1] - x_[j-1])\n","            # take last min from a square box\n","            last_min = np.min([dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1]])\n","            dtw_matrix[i, j] = cost + last_min\n","    return dtw_matrix[n][m]\n","\n","def unroll_signal(x):\n","    x = np.array(x).reshape(100)\n","    return np.median(x)\n","\n","def prune_false_positive(is_anomaly, anomaly_score, change_threshold):\n","    #The model might detect a high number of false positives.\n","    #In such a scenario, pruning of the false positive is suggested.\n","    #Method used is as described in the Section 5, part D Identifying Anomalous\n","    #Sequence, sub-part - Mitigating False positives\n","    #TODO code optimization\n","    seq_details = []\n","    delete_sequence = 0\n","    start_position = 0\n","    end_position = 0\n","    max_seq_element = anomaly_score[0]\n","    for i in range(1, len(is_anomaly)):\n","        if i+1 == len(is_anomaly):\n","            seq_details.append([start_position, i, max_seq_element, delete_sequence])\n","        elif is_anomaly[i] == 1 and is_anomaly[i+1] == 0:\n","            end_position = i\n","            seq_details.append([start_position, end_position, max_seq_element, delete_sequence])\n","        elif is_anomaly[i] == 1 and is_anomaly[i-1] == 0:\n","            start_position = i\n","            max_seq_element = anomaly_score[i]\n","        if is_anomaly[i] == 1 and is_anomaly[i-1] == 1 and anomaly_score[i] > max_seq_element:\n","            max_seq_element = anomaly_score[i]\n","\n","    max_elements = list()\n","    for i in range(0, len(seq_details)):\n","        max_elements.append(seq_details[i][2])\n","\n","    max_elements.sort(reverse=True)\n","    max_elements = np.array(max_elements)\n","    change_percent = abs(max_elements[1:] - max_elements[:-1]) / max_elements[1:]\n","\n","    #Appending 0 for the 1 st element which is not change percent\n","    delete_seq = np.append(np.array([0]), change_percent < change_threshold)\n","\n","    #Mapping max element and seq details\n","    for i, max_elt in enumerate(max_elements):\n","        for j in range(0, len(seq_details)):\n","            if seq_details[j][2] == max_elt:\n","                seq_details[j][3] = delete_seq[i]\n","\n","    for seq in seq_details:\n","        if seq[3] == 1: #Delete sequence\n","            is_anomaly[seq[0]:seq[1]+1] = [0] * (seq[1] - seq[0] + 1)\n"," \n","    return is_anomaly\n","\n","def detect_anomaly(anomaly_score):\n","    window_size = len(anomaly_score) // 3\n","    step_size = len(anomaly_score) // (3 * 10)\n","\n","    is_anomaly = np.zeros(len(anomaly_score))\n","\n","    for i in range(0, len(anomaly_score) - window_size, step_size):\n","        window_elts = anomaly_score[i:i+window_size]\n","        window_mean = np.mean(window_elts)\n","        window_std = np.std(window_elts)\n","\n","        for j, elt in enumerate(window_elts):\n","            if (window_mean - 3 * window_std) < elt < (window_mean + 3 * window_std):\n","                is_anomaly[i + j] = 0\n","            else:\n","                is_anomaly[i + j] = 1\n","\n","    return is_anomaly\n","\n","def find_scores(y_true, y_predict):\n","    print(sum(y_predict))\n","    tp = tn = fp = fn = 0\n","\n","    for i in range(0, len(y_true)):\n","        if y_true[i] == 1 and y_predict[i] == 1:\n","            tp += 1\n","        elif y_true[i] == 1 and y_predict[i] == 0:\n","            fn += 1\n","        elif y_true[i] == 0 and y_predict[i] == 0:\n","            tn += 1\n","        elif y_true[i] == 0 and y_predict[i] == 1:\n","            fp += 1\n","\n","    print ('Accuracy {:.2f}'.format((tp + tn)/(len(y_true))))\n","    precision = tp / (tp + fp)\n","    recall = tp / (tp + fn)\n","    print ('Precision {:.2f}'.format(precision))\n","    print ('Recall {:.2f}'.format(recall))\n","    print ('F1 Score {:.2f}'.format(2 * precision * recall / (precision + recall)))"],"metadata":{"id":"PuJY1NGfLeZ3","executionInfo":{"status":"ok","timestamp":1666224832961,"user_tz":-540,"elapsed":5,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"bUG1D9-CSqQy"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"hNPCXH550HNG","executionInfo":{"status":"ok","timestamp":1666224832961,"user_tz":-540,"elapsed":4,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":["import os\n","import logging\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","\n","class Encoder(nn.Module):\n","\n","  def __init__(self, encoder_path, signal_shape=100):\n","    super(Encoder, self).__init__()\n","    self.signal_shape = signal_shape\n","    self.lstm = nn.LSTM(input_size=self.signal_shape, hidden_size=20, num_layers=1, bidirectional=True)\n","    self.dense = nn.Linear(in_features=40, out_features=20)\n","    self.encoder_path = encoder_path\n","\n","  def forward(self, x):\n","    x = x.view(1, 64, self.signal_shape).float()\n","    x, (hn, cn) = self.lstm(x)\n","    x = self.dense(x)\n","    return (x)\n","\n","class Decoder(nn.Module):\n","  def __init__(self, decoder_path, signal_shape=100):\n","    super(Decoder, self).__init__()\n","    self.signal_shape = signal_shape\n","    self.lstm = nn.LSTM(input_size=20, hidden_size=64, num_layers=2, bidirectional=True)\n","    self.dense = nn.Linear(in_features=128, out_features=self.signal_shape)\n","    self.decoder_path = decoder_path\n","\n","  def forward(self, x):\n","    x, (hn, cn) = self.lstm(x)\n","    x = self.dense(x)\n","    return (x)\n","\n","class CriticX(nn.Module):\n","  def __init__(self, critic_x_path, signal_shape=100):\n","    super(CriticX, self).__init__()\n","    self.signal_shape = signal_shape\n","    self.dense1 = nn.Linear(in_features=self.signal_shape, out_features=20)\n","    self.dense2 = nn.Linear(in_features=20, out_features=1)\n","    self.critic_x_path = critic_x_path\n","\n","  def forward(self, x):\n","    x = x.view(1, 64, self.signal_shape).float()\n","    # x = x.view(1, x.shape[1], self.signal_shape).float()\n","    x = self.dense1(x)\n","    x = self.dense2(x)\n","    return (x)\n","\n","class CriticZ(nn.Module):\n","  def __init__(self, critic_z_path):\n","    super(CriticZ, self).__init__()\n","    self.dense1 = nn.Linear(in_features=20, out_features=1)\n","    self.critic_z_path = critic_z_path\n","\n","  def forward(self, x):\n","    x = self.dense1(x)\n","    return (x)\n","\n","def unroll_signal(self, x):\n","  x = np.array(x).reshape(100)\n","  return np.median(x)\n","\n","def test(self):\n","  \"\"\"\n","  Returns a dataframe with original value, reconstructed value, reconstruction error, critic score\n","  \"\"\"\n","  df = self.test_dataset.copy()\n","  X_ = list()\n","\n","  RE = list()  #Reconstruction error\n","  CS = list()  #Critic score\n","\n","  for i in range(0, df.shape[0]):\n","    x = df.rolled_signal[i]\n","    x = tf.reshape(x, (1, 100, 1))\n","    z = encoder(x)\n","    z = tf.expand_dims(z, axis=2)\n","    x_ = decoder(z)\n","\n","    re = dtw_reconstruction_error(tf.squeeze(x_).numpy(), tf.squeeze(x).numpy()) #reconstruction error\n","    cs = critic_x(x)\n","    cs = tf.squeeze(cs).numpy()\n","    RE.append(re)\n","    CS.append(cs)\n","\n","    x_ = unroll_signal(x_)\n","\n","    X_.append(x_)\n","\n","  df['generated_signals'] = X_\n","\n","  return df"]},{"cell_type":"code","source":["'''\n","https://github.com/arunppsg/TadGAN\n","'''\n","def critic_x_iteration(sample):\n","  optim_cx.zero_grad()\n","  \n","  # x = sample['signal'].view(1, batch_size, signal_shape)\n","  x = sample.view(1, sample.shape[0], signal_shape)\n","  valid_x = critic_x(x)\n","  valid_x = torch.squeeze(valid_x)\n","  critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) # Wasserstein Loss\n","\n","  #The sampled z are the anomalous points - points deviating from actual distribution of z (obtained through encoding x)\n","  z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n","  x_ = decoder(z)\n","  fake_x = critic_x(x_)\n","  fake_x = torch.squeeze(fake_x)\n","  critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)  #Wasserstein Loss\n","\n","  alpha = torch.rand(x.shape)\n","  ix = Variable(alpha * x + (1 - alpha) * x_) #Random Weighted Average\n","  ix.requires_grad_(True)\n","  v_ix = critic_x(ix)\n","  v_ix.mean().backward()\n","  gradients = ix.grad\n","  #Gradient Penalty Loss\n","  gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n","\n","  #Critic has to maximize Cx(Valid X) - Cx(Fake X).\n","  #Maximizing the above is same as minimizing the negative.\n","  wl = critic_score_fake_x - critic_score_valid_x\n","  loss = wl + gp_loss\n","  loss.backward()\n","  optim_cx.step()\n","\n","  return loss\n","\n","def critic_z_iteration(sample):\n","  optim_cz.zero_grad()\n","\n","  # x = sample['signal'].view(1, batch_size, signal_shape)\n","  x = sample.view(1, batch_size, signal_shape)\n","  z = encoder(x)\n","  valid_z = critic_z(z)\n","  valid_z = torch.squeeze(valid_z)\n","  critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n","\n","  z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n","  fake_z = critic_z(z_)\n","  fake_z = torch.squeeze(fake_z)\n","  critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z) #Wasserstein Loss\n","\n","  wl = critic_score_fake_z - critic_score_valid_z\n","\n","  alpha = torch.rand(z.shape)\n","  iz = Variable(alpha * z + (1 - alpha) * z_) #Random Weighted Average\n","  iz.requires_grad_(True)\n","  v_iz = critic_z(iz)\n","  v_iz.mean().backward()\n","  gradients = iz.grad\n","  gp_loss = torch.sqrt(torch.sum(torch.square(gradients).view(-1)))\n","\n","  loss = wl + gp_loss\n","  loss.backward()\n","  optim_cz.step()\n","\n","  return loss\n","\n","def encoder_iteration(sample):\n","  optim_enc.zero_grad()\n","\n","  # x = sample['signal'].view(1, batch_size, signal_shape)\n","  x = sample.view(1, batch_size, signal_shape)\n","  valid_x = critic_x(x)\n","  valid_x = torch.squeeze(valid_x)\n","  critic_score_valid_x = torch.mean(torch.ones(valid_x.shape) * valid_x) #Wasserstein Loss\n","\n","  z = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n","  x_ = decoder(z)\n","  fake_x = critic_x(x_)\n","  fake_x = torch.squeeze(fake_x)\n","  critic_score_fake_x = torch.mean(torch.ones(fake_x.shape) * fake_x)\n","\n","  enc_z = encoder(x)\n","  gen_x = decoder(enc_z)\n","\n","  mse = mse_loss(x.float(), gen_x.float())\n","  loss_enc = mse + critic_score_valid_x - critic_score_fake_x\n","  loss_enc.backward(retain_graph=True)\n","  optim_enc.step()\n","\n","  return loss_enc\n","\n","def decoder_iteration(sample):\n","  optim_dec.zero_grad()\n","\n","  # x = sample['signal'].view(1, batch_size, signal_shape)\n","  x = sample.view(1, batch_size, signal_shape)\n","  z = encoder(x)\n","  valid_z = critic_z(z)\n","  valid_z = torch.squeeze(valid_z)\n","  critic_score_valid_z = torch.mean(torch.ones(valid_z.shape) * valid_z)\n","\n","  z_ = torch.empty(1, batch_size, latent_space_dim).uniform_(0, 1)\n","  fake_z = critic_z(z_)\n","  fake_z = torch.squeeze(fake_z)\n","  critic_score_fake_z = torch.mean(torch.ones(fake_z.shape) * fake_z)\n","\n","  enc_z = encoder(x)\n","  gen_x = decoder(enc_z)\n","\n","  mse = mse_loss(x.float(), gen_x.float())\n","  loss_dec = mse + critic_score_valid_z - critic_score_fake_z\n","  loss_dec.backward(retain_graph=True)\n","  optim_dec.step()\n","\n","  return loss_dec\n","\n","\n","def train(n_epochs=2000):\n","  logging.debug('Starting training')\n","  cx_epoch_loss = list()\n","  cz_epoch_loss = list()\n","  encoder_epoch_loss = list()\n","  decoder_epoch_loss = list()\n","\n","  for epoch in range(n_epochs):\n","    logging.debug('Epoch {}'.format(epoch))\n","    n_critics = 5\n","\n","    cx_nc_loss = list()\n","    cz_nc_loss = list()\n","\n","    for i in range(n_critics):\n","      cx_loss = list()\n","      cz_loss = list()\n","\n","      for batch, sample in enumerate(train_loader):\n","        loss = critic_x_iteration(sample)\n","        cx_loss.append(loss)\n","\n","        loss = critic_z_iteration(sample)\n","        cz_loss.append(loss)\n","\n","      cx_nc_loss.append(torch.mean(torch.tensor(cx_loss)))\n","      cz_nc_loss.append(torch.mean(torch.tensor(cz_loss)))\n","\n","    logging.debug('Critic training done in epoch {}'.format(epoch))\n","    encoder_loss = list()\n","    decoder_loss = list()\n","\n","    for batch, sample in enumerate(train_loader):\n","      enc_loss = encoder_iteration(sample)\n","      dec_loss = decoder_iteration(sample)\n","      encoder_loss.append(enc_loss)\n","      decoder_loss.append(dec_loss)\n","\n","    cx_epoch_loss.append(torch.mean(torch.tensor(cx_nc_loss)))\n","    cz_epoch_loss.append(torch.mean(torch.tensor(cz_nc_loss)))\n","    encoder_epoch_loss.append(torch.mean(torch.tensor(encoder_loss)))\n","    decoder_epoch_loss.append(torch.mean(torch.tensor(decoder_loss)))\n","    logging.debug('Encoder decoder training done in epoch {}'.format(epoch))\n","    logging.debug('critic x loss {:.3f} critic z loss {:.3f} \\nencoder loss {:.3f} decoder loss {:.3f}\\n'.format(cx_epoch_loss[-1], cz_epoch_loss[-1], encoder_epoch_loss[-1], decoder_epoch_loss[-1]))\n","\n","    if epoch % 10 == 0:\n","      torch.save(encoder.state_dict(), encoder.encoder_path)\n","      torch.save(decoder.state_dict(), decoder.decoder_path)\n","      torch.save(critic_x.state_dict(), critic_x.critic_x_path)\n","      torch.save(critic_z.state_dict(), critic_z.critic_z_path)\n"],"metadata":{"id":"Z4EblAP8W_A9","executionInfo":{"status":"ok","timestamp":1666224832961,"user_tz":-540,"elapsed":4,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"sPmDDw98C298","executionInfo":{"status":"ok","timestamp":1666224832961,"user_tz":-540,"elapsed":4,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## Main"],"metadata":{"id":"9TSMXPMBStSQ"}},{"cell_type":"code","source":["# dataset = pd.read_csv('exchange-2_cpc_results.csv')\n","# #Splitting intro train and test\n","# #TODO could be done in a more pythonic way\n","# train_len = int(0.7 * dataset.shape[0])\n","# dataset[0:train_len].to_csv('train_dataset.csv', index=False)\n","# dataset[train_len:].to_csv('test_dataset.csv', index=False)\n","\n","# train_dataset = SignalDataset(path='train_dataset.csv')\n","# test_dataset = SignalDataset(path='test_dataset.csv')\n","\n","config = easydict.EasyDict({\n","    \"num_epochs\" : 10, #500\n","    \"batch_size\" : 64, #16 \n","    \"mode\" : 'train',\n","    # \"mode\" : \"test\",\n","    \"lr\" : 5e-3, \n","    \"wd\" : None,\n","    \"window_size\" : 1000,\n","    \"stride\" : 250,\n","    \"threshold\" : 0.3, # 0.3이나 0.2로 하기\n","    \"seed\" : 1004,\n","    \"device\" : torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n","    \"small_window\" : 9,\n","    \"small_stride\" : 5,\n","    \"latent\" : 20\n","})\n","\n","batch_size = 64\n","lr = 5e-3 #1e-6\n","signal_shape = 1000 #100\n","latent_space_dim = 20\n","\n","#---# Dataset & Dataloader #---#\n","train_dataset = MyDataset(config, mode=\"train\")\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True) # drop_last=True\n","\n","val_dataset = MyDataset(config, mode=\"val\")\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","test_dataset = MyDataset(config, mode=\"test\")\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True) # drop_last=True\n","\n","logging.info('Number of train datapoints is {}'.format(len(train_dataset)))\n","logging.info('Number of samples in train dataset {}'.format(len(train_dataset)))\n","\n","encoder_path = save_path + '/model_save/encoder.pt'\n","decoder_path = save_path + '/model_save/decoder.pt'\n","critic_x_path = save_path + '/model_save/critic_x.pt'\n","critic_z_path = save_path + '/model_save/critic_z.pt'\n","\n","encoder = Encoder(encoder_path, signal_shape)\n","decoder = Decoder(decoder_path, signal_shape)\n","critic_x = CriticX(critic_x_path, signal_shape)\n","critic_z = CriticZ(critic_z_path)\n","\n","mse_loss = torch.nn.MSELoss()\n","\n","optim_enc = optim.Adam(encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n","optim_dec = optim.Adam(decoder.parameters(), lr=lr, betas=(0.5, 0.999))\n","optim_cx = optim.Adam(critic_x.parameters(), lr=lr, betas=(0.5, 0.999))\n","optim_cz = optim.Adam(critic_z.parameters(), lr=lr, betas=(0.5, 0.999))\n","\n","train(n_epochs=10)\n","\n","\n","\n","def sliding_window(arr):\n","    start_pt = 0\n","    total_data = []\n","    \n","    while(True):\n","      if len(arr) < (start_pt + config.window_size) : break\n","      data = arr[start_pt:start_pt + config.window_size]\n","      start_pt += config.stride\n","      total_data.append(data)\n","\n","    return np.array(total_data)\n","    \n","true_label = get_true_label(test_dataset)\n","true_slided = sliding_window(true_label)\n","true_labeling = []\n","for i in true_slided:\n","  avg = sum(i) / len(i)\n","  if avg > 0.5:\n","    true_labeling.append(1)\n","  else :\n","    true_labeling.append(0)\n","\n","\n","\n","ad_test(test_loader, encoder, decoder, critic_x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"BSklUucaYRHF","executionInfo":{"status":"error","timestamp":1666224856755,"user_tz":-540,"elapsed":23798,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"fdd018fa-940b-4edf-9c86-d4442ef92f00"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["(896,)\n","0.0\n","Accuracy 0.95\n"]},{"output_type":"error","ename":"ZeroDivisionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-e193050ac6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mad_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-81964a0212e8>\u001b[0m in \u001b[0;36mad_test\u001b[0;34m(test_loader, encoder, decoder, critic_x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_labeling\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mfind_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#Other error metrics - point wise difference, Area difference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-81964a0212e8>\u001b[0m in \u001b[0;36mfind_scores\u001b[0;34m(y_true, y_predict)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Precision {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]},{"cell_type":"code","source":["test_dataset.x_test.shape"],"metadata":{"id":"i9-fmGKpn_Wl","executionInfo":{"status":"aborted","timestamp":1666224856756,"user_tz":-540,"elapsed":14,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum(true_labeling)"],"metadata":{"id":"7oRd7ubFuJ-b","executionInfo":{"status":"aborted","timestamp":1666224856756,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(output)"],"metadata":{"id":"Vpz8UYV_rAhX","executionInfo":{"status":"aborted","timestamp":1666224856756,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qxb1cyeOrAfJ","executionInfo":{"status":"aborted","timestamp":1666224856757,"user_tz":-540,"elapsed":14,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LFrJiTjBrAcs","executionInfo":{"status":"aborted","timestamp":1666224856757,"user_tz":-540,"elapsed":14,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sliding_window(arr):\n","    start_pt = 0\n","    total_data = []\n","    \n","    while(True):\n","      if len(arr) < (start_pt + config.window_size) : break\n","      data = arr[start_pt:start_pt + config.window_size]\n","      start_pt += config.stride\n","      total_data.append(data)\n","\n","    return np.array(total_data)\n","    \n","true_label = get_true_label(test_dataset)\n","true_slided = sliding_window(true_label)\n","true_labeling = []\n","for i in true_slided:\n","  avg = sum(i) / len(i)\n","  if avg > 0.3:\n","    true_labeling.append(1)\n","  else :\n","    true_labeling.append(0)"],"metadata":{"id":"NXVb9OS_eapK","executionInfo":{"status":"aborted","timestamp":1666224856757,"user_tz":-540,"elapsed":14,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(true_labeling)"],"metadata":{"id":"G-Dx9FABiuW5","executionInfo":{"status":"aborted","timestamp":1666224856757,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sliding_window(arr):\n","    start_pt = 0\n","    total_data = []\n","    \n","    while(True):\n","      if len(arr) < (start_pt + config.window_size) : break\n","      data = arr[start_pt:start_pt + config.window_size]\n","      start_pt += config.stride\n","      total_data.append(data)\n","\n","    return np.array(total_data)"],"metadata":{"id":"6Dp8AFyLsi6A","executionInfo":{"status":"aborted","timestamp":1666224856757,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset.x_test.shape"],"metadata":{"id":"ant_Tizsn_Nd","executionInfo":{"status":"aborted","timestamp":1666224856758,"user_tz":-540,"elapsed":14,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7W5xXsq56mT","executionInfo":{"status":"aborted","timestamp":1666224856758,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":["''''config = easydict.EasyDict({\n","    \"num_epochs\" : 10, #500\n","    \"batch_size\" : 16, #16 \n","    \"mode\" : 'train',\n","    # \"mode\" : \"test\",\n","    \"lr\" : 5e-3, \n","    \"wd\" : None,\n","    \"window_size\" : 1000,\n","    \"stride\" : 250,\n","    \"threshold\" : 0.3, # 0.3이나 0.2로 하기\n","    \"seed\" : 1004,\n","    \"device\" : torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n","    \"small_window\" : 9,\n","    \"small_stride\" : 5,\n","    \"latent\" : 20\n","})\n","starttime = time.time()\n","seed_everything(config.seed)\n","save_df = pd.DataFrame(columns = ['epochs', 'batch', 'window_size', 'stride', \n","                                  'small_window', 'small_stride', \n","                                  'recall', 'precision', 'accuracy',\n","                                  'latent']); df_idx = 0\n","\n","# tm = time.localtime(time.time())\n","# string = time.strftime('%Y%m%d_%H%M%S', tm)\n","# wandb.init(project=\"Anomaly-Oil\", entity=\"sohyun\", name=string, magic=True)\n","\n","# model = RecurrentAutoencoder(config.window_size, 1, 20).to(config.device) # timesteps, n_features, 128 #원래\n","for sw in [900, 1000, 1100, 1200]: # 3, 5, 7, 8, 9, 10\n","  for ss in [200, 250, 300, 350]: # 3, 4, 5\n","    if sw <= ss : continue\n","    # config.small_window = sw\n","    # config.small_stride = ss\n","    config.window_size = sw\n","    config.stride = ss\n","    print(f\"\\n< small_window : {sw}, small_stride : {ss} >\")\n","    timesteps2 = math.ceil((config.window_size - config.small_window) / config.small_stride)\n","\n","    model = RecurrentAutoencoder(config, timesteps2, config.small_window, config.latent).to(config.device)\n","    model, history = train_model(config, model)\n","\n","    MODEL_PATH = './lstm_ae_model.pth'\n","    torch.save(model, MODEL_PATH)\n","    model = torch.load(MODEL_PATH)\n","    model = model.to(config.device)\n","\n","    test_dataset, predictions, losses = predict(config, model)\n","    true_label = get_true_label(test_dataset)\n","    predictions = np.concatenate(predictions).squeeze()\n","    sns.distplot(losses, bins=50, kde=True)\n","\n","    #---# calculate error range #---#\n","    errors, predictions_vs = reconstruction_errors(test_dataset.x_test, predictions, score_window=config.window_size, step_size=1) #score_window=config.window_size\n","    error_range = find_anomalies(errors, index=range(len(errors)), anomaly_padding=5)\n","    print(\"=== error_range ===\\n\", error_range)\n","    prediction = pd.DataFrame(0, index=range(0, len(test_dataset.scaled_test)), columns={'Fu'}) # 빈 행렬\n","    for i in error_range:\n","      start = int(i[0])\n","      end = int(i[1])\n","      prediction[start:end] = 1\n","\n","    anomaly_value = get_anomaly_time(test_dataset.scaled_test, prediction['Fu'].values.tolist())\n","    drawing(anomaly_value, pd.DataFrame(test_dataset.scaled_test))\n","    recall, precision, accuracy = calculate(pd.DataFrame(true_label), pd.DataFrame(anomaly_value))\n","    print(f'recall : {recall}, precision : {precision}, accuracy : {accuracy}')\n","    save_df.loc[df_idx] = [config.num_epochs, config.batch_size, \n","                          config.window_size, config.stride, \n","                          config.small_window, config.small_stride, \n","                          recall, precision, accuracy, config.latent]\n","    df_idx = df_idx + 1\n","    save_df.to_csv(f'./LSTMAE_10061045.csv', header=True, index=False)\n","endtime = time.time()\n","print(f\"time : {endtime - starttime} sec!!!!!!\") # end''''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kP08mLBS06Jp","executionInfo":{"status":"aborted","timestamp":1666224856758,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":["'''def train_model(config, model):\n","  # optimizer = torch.optim.Adam(params = model.parameters(), lr = config.lr) # lr = config.lr\n","  # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n","\n","  optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n","  criterion = nn.L1Loss(reduction='sum').to(config.device)\n","  history = dict(train=[], val=[])\n","  best_model_wts = copy.deepcopy(model.state_dict())\n","  best_loss = 1000000.0\n","  print(\"start!\")\n","  \n","  train_dataset = MyDataset(config, mode=\"train\")\n","  tr_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False) # True\n","  val_dataset = MyDataset(config, mode=\"val\")\n","  va_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n","  # va_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n","\n","  for epoch in range(config.num_epochs):\n","    model = model.train()\n","    train_losses = []\n","    \n","    for batch_idx, batch_x in enumerate(tr_dataloader):\n","      optimizer.zero_grad()\n","      batch_x_tensor = batch_x.to(config.device)\n","      seq_pred = model(batch_x_tensor).squeeze()\n","\n","      loss = criterion(seq_pred, batch_x_tensor)\n","      loss.backward()\n","      optimizer.step()\n","      train_losses.append(loss.item())\n","\n","    val_losses = []\n","    model = model.eval()\n","    with torch.no_grad():\n","      va_x = next(va_dataloader.__iter__())\n","      va_x_tensor = va_x.to(config.device)\n","      seq_pred = model(va_x_tensor).squeeze()\n","      loss = criterion(seq_pred, va_x_tensor)\n","      val_losses.append(loss.item())\n","\n","    train_loss = np.mean(train_losses)\n","    val_loss = np.mean(val_losses)\n","    history['train'].append(train_loss)\n","    history['val'].append(val_loss)\n","    if val_loss < best_loss:\n","      print(f\"epoch {epoch}\")\n","      best_loss = val_loss\n","      best_model_wts = copy.deepcopy(model.state_dict())\n","    print(f'Epoch {epoch+1}: train loss {train_loss} val loss {val_loss}')\n","\n","  model.load_state_dict(best_model_wts)\n","  return model.eval(), history\n","\n","def predict(config, model):\n","  predictions, losses = [], []\n","  criterion = nn.L1Loss(reduction='sum').to(config.device)\n","  test_dataset = MyDataset(config, mode=\"test\")\n","  dataloader_ae = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n","  \n","  with torch.no_grad():\n","    model = model.eval()\n","    if config.batch_size == len(test_dataset):\n","      seq_true = next(dataloader_ae.__iter__())\n","      seq_true = seq_true.to(config.device)\n","      seq_pred = model(seq_true).squeeze()\n","      loss = criterion(seq_pred, seq_true)\n","      predictions.append(seq_pred.cpu().numpy().flatten())\n","      losses.append(loss.item())\n","    else :\n","      for idx, seq_true in enumerate(dataloader_ae):\n","        seq_true = seq_true.to(config.device)\n","        seq_pred = model(seq_true) # shape : [batch, window_size]\n","        loss = criterion(seq_pred.squeeze(), seq_true)\n","        # predictions.append(seq_pred.cpu().numpy().flatten())\n","        predictions.append(seq_pred.cpu().numpy())\n","        losses.append(loss.item())\n","  return test_dataset, predictions, losses'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHldolkTwZBz","executionInfo":{"status":"aborted","timestamp":1666224856758,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2jd8bWocNzA","executionInfo":{"status":"aborted","timestamp":1666224856758,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud1EwYw4cNvj","executionInfo":{"status":"aborted","timestamp":1666224856758,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQoQOOtBhcXl","executionInfo":{"status":"aborted","timestamp":1666224856759,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK3w5Cr8v5dJ","executionInfo":{"status":"aborted","timestamp":1666224856759,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vA2G0vWmv5Wo","executionInfo":{"status":"aborted","timestamp":1666224856759,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gY-C_kr7v5TQ","executionInfo":{"status":"aborted","timestamp":1666224856759,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbMQF9uNv5Oz","executionInfo":{"status":"aborted","timestamp":1666224856759,"user_tz":-540,"elapsed":13,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubS_Nsgqv5Ke","executionInfo":{"status":"aborted","timestamp":1666224856760,"user_tz":-540,"elapsed":14,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["bUG1D9-CSqQy"],"provenance":[],"mount_file_id":"1iUI5TdFKacTS2eemlzXXKidMN4qhQqHU","authorship_tag":"ABX9TyPstjErDhp28ej8o7ozYjFT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}