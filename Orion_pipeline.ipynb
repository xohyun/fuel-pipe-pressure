{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XltI_t_kUGvD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663027937542,"user_tz":-540,"elapsed":2090,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"b0b23c69-dcc0-40c5-cc4c-07a9843aef0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/.shortcut-targets-by-id/1j1N0u5t0l99N_wfSd5UZvnhugzn5g_NC/TimeSeriesAnomaly/data/modify\n"]}],"source":["from google.colab import drive\n","# drive.mount('/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data')\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/IITP/sohyun/TimeSeriesAnomaly/data/modify"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qi5fjXQLHQab"},"outputs":[],"source":["# !pip install wandb -qqq\n","# import wandb\n","# wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gq_QRBw-_yqo"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import easydict\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","import random\n","import pandas as pd\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","import time\n","import math"]},{"cell_type":"markdown","metadata":{"id":"zjoJxI3iylhf"},"source":["## Data "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yH_ixp0aWdm4"},"outputs":[],"source":["#---# LOAD npy file #---#\n","Fu_20_normal = np.load('Fu_20_normal.npy')\n","Fu_21_normal = np.load('Fu_21_normal.npy')\n","Fu_21_abnormal = np.load('Fu_21_abnormal.npy')\n","Fu_22_normal = np.load('Fu_22_normal.npy')\n","Fu_22_abnormal = np.load('Fu_22_abnormal.npy')\n","\n","Fu_20_normal_10 = np.load('Fu_20_normal_10.npy')\n","Fu_21_normal_10 = np.load('Fu_21_normal_10.npy')\n","Fu_21_abnormal_10 = np.load('Fu_21_abnormal_10.npy')\n","Fu_22_normal_10 = np.load('Fu_22_normal_10.npy')\n","Fu_22_abnormal_10 = np.load('Fu_22_abnormal_10.npy')\n","\n","# import sys\n","# np.set_printoptions(threshold=sys.maxsize) # print all\n","\n","#---# 확인용 #---#\n","# plt.figure(figsize=(30,5))\n","# plt.plot(Fu_22_abnormal_10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fS479XwTHmk0"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","class MyDataset(Dataset):\n","  def __init__(self, data):\n","    self.data = np.array(self.sliding_window(data, config.window_size, config.stride))\n","    self.data = self.data.reshape(-1, config.window_size)\n","    \n","  def __getitem__(self, index):\n","    self.x = self.data[index]\n","    return (index, torch.Tensor(self.x))\n","      \n","  def __len__(self):\n","    return len(self.data)\n","\n","  #---# Window #---#\n","  def sliding_window(self, arr, window_size, stride):\n","    start_pt = 0\n","    total_data = []\n","    while(True) :\n","      if len(arr) < (start_pt + window_size) : break\n","      data = arr[start_pt:start_pt+window_size]\n","      start_pt += stride\n","      total_data.append(data)\n","    return total_data\n","\n","\n","def drawing(pred, x) :\n","  #---# Drawing - 22 #---#\n","  plt.figure(figsize=(30,5))\n","  plt.plot(x, markersize=1)\n","  plt.plot(pred, marker='.', markersize=2, color='r', linestyle='None')\n","\n","  '''\n","  for i in range(len(pred)):\n","    if pred[i] == 1:\n","      # plt.vlines(i * config.window_size, -1, 4, colors=\"y\")\n","      # plt.vlines((i+1) * config.window_size, -1, 4, colors=\"y\")\n","      before = i * config.window_size\n","      after = (i+1) * config.window_size\n","\n","      r = np.linspace(before, after)\n","      plt.fill_between(r, -1, 4, color = \"yellow\", alpha = 0.5)\n","  '''\n","\n","  #---# 실제 anomaly 값 구간 #---#\n","  a = np.linspace(62200, 65300)\n","  # plt.fill_between(a, 0, 2000, color='green', alpha=0.3)\n","  plt.fill_between(a, -1, 4, color='green', alpha=0.5)\n","  b = np.linspace(95600, 99300)\n","  # plt.fill_between(b, 0, 2000, color='green', alpha=0.5)\n","  plt.fill_between(b, -1, 4, color='green', alpha=0.5)\n","  c = np.linspace(148400, 152300)\n","  # plt.fill_between(c, 0, 2000, color='green', alpha=0.5)\n","  plt.fill_between(c, -1, 4, color='green', alpha=0.5)\n","\n","  plt.show()\n","  plt.clf()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCqpW85kT5n8"},"outputs":[],"source":["config = easydict.EasyDict({\n","    \"num_epochs\" : 500, #500\n","    \"batch_size\" : 16, #16 \n","    \"mode\" : 'train',\n","    # \"mode\" : \"test\",\n","    \"lr\" : 1e-3, \n","    \"wd\" : None,\n","    \"window_size\" : 1000,\n","    \"stride\" : 250,\n","    \"threshold\" : 0.05 # 0.3이나 0.2로 하기\n","})\n","\n","tm = time.localtime(time.time())\n","string = time.strftime('%Y%m%d_%H%M%S', tm)\n","\n","# wandb.init(project=\"Anomaly-Oil\", entity=\"sohyun\", name=string, magic=True)\n","\n","train = pd.DataFrame(Fu_22_normal_10, columns=['Fu'])\n","train = pd.concat([train, pd.DataFrame(Fu_21_normal_10, columns=['Fu']), pd.DataFrame(Fu_20_normal_10, columns=['Fu'])], axis=0)\n","test = pd.DataFrame(Fu_22_abnormal_10, columns=['Fu'])\n","test_len = len(test)\n","total = pd.DataFrame(pd.concat([train, test], axis=0))\n","\n","# train_scale = train['Fu'].values\n","# test_scale = test['Fu'].values\n","# total_scale = total['Fu'].values\n","\n","#---# Noramlize #---#\n","scaler = StandardScaler()\n","total_scale = scaler.fit_transform(total); total_scale = pd.DataFrame(total_scale, columns=['Fu'])['Fu'].values.tolist() # total 먼저 해놓고 transform\n","train_scale = scaler.transform(train); train_scale = pd.DataFrame(train_scale, columns=['Fu'])['Fu'].values.tolist()\n","test_scale = scaler.transform(test); test_scale = pd.DataFrame(test_scale, columns=['Fu'])['Fu'].values.tolist()\n","\n","# scaler = MinMaxScaler()\n","# total_scale = scaler.fit_transform(total); total_scale = pd.DataFrame(total_scale, columns=['Fu'])['Fu'].values.tolist()\n","# test_scale = scaler.transform(test); test_scale = pd.DataFrame(test_scale, columns=['Fu'])['Fu'].values.tolist()\n","\n","#---# Setting train data #---# 나머지 끊어주어야 함\n","train_dataset1 = MyDataset(data=train_scale[:len(Fu_22_normal_10)])\n","train_dataset2 = MyDataset(data=train_scale[len(Fu_22_normal_10):(len(Fu_22_normal_10)+len(Fu_21_normal_10))])\n","train_dataset3 = MyDataset(data=train_scale[(len(Fu_22_normal_10)+len(Fu_21_normal_10)):])\n","train_dataset = np.concatenate([train_dataset1.data, train_dataset2.data, train_dataset3.data], axis = 0)\n","\n","train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n","test_dataset = MyDataset(data=test_scale)\n","test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n","\n","\n","# optimizer = torch.optim.Adam(params = model.parameters(), lr = config.lr) # lr = config.lr\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qI9EZHR4z-Fg"},"outputs":[],"source":["!pip install orion-ml -qqq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5qbpzacHjdU"},"outputs":[],"source":["####################\n","#---# 예제 데이터 #---#\n","####################\n","# from orion.data import load_signal\n","\n","# train_data = load_signal('S-1-train')\n","# # timestamp2 = np.arange(0, len(train_data)/100, 0.01)\n","# # train_data['timestamp'] = timestamp2\n","\n","# from orion import Orion\n","# hyperparameters = {\n","#     'keras.Sequential.LSTMTimeSeriesRegressor#1' : {\n","#         'epochs' : 3,\n","#         'verbose' : True\n","#     }\n","# }\n","# orion = Orion(\n","#     pipeline = 'lstm_dynamic_threshold',\n","#     hyperparameters = hyperparameters\n","# )\n","# orion.fit(train_data)\n","# new_data = load_signal('S-1-new')\n","# anomalies = orion.detect(new_data)\n","# print(anomalies)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4auc0uljs6gV"},"outputs":[],"source":["from pandas.tseries.offsets import Second\n","from datetime import date, timedelta\n","from orion import Orion\n","from datetime import datetime\n","\n","def make_timestamp(date, data_len) :\n","  # start_date = datetime.strptime('201802210000', '%Y%m%d%H%M')\n","  start_date = datetime.strptime(date, '%Y%m%d%H%M')\n","  timestamp = time.mktime(start_date.timetuple())\n","  dates = []; dates.append(timestamp)\n","\n","  for i in range(data_len-1) :\n","    new_start_date = start_date + timedelta(minutes=10)\n","    timestamp = time.mktime(new_start_date.timetuple())\n","    dates.append(timestamp)\n","    start_date = new_start_date\n","  \n","  return dates\n","\n","dates = make_timestamp('201802210000', len(train_scale))\n","traindata = pd.DataFrame({\"timestamp\":dates, \"value\":train_scale})\n","dates_test = make_timestamp('201802211200', len(test_scale))\n","testdata = pd.DataFrame({\"timestamp\":dates_test, \"value\":test_scale})\n","\n","hyperparameters = {\n","    \"mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n","      \"target_column\": 0,\n","      \"window_size\": 1000,\n","      \"step_size\": 250\n","    },\n","    \"numpy.reshape#1\": {\n","        \"newshape\": [\n","            -1,\n","            1000\n","        ]\n","    },\n","    \"statsmodels.tsa.arima_model.Arima#1\": {\n","          \"steps\": 2,\n","          \"p\" : 2\n","    },\n","    \"orion.primitives.timeseries_anomalies.find_anomalies#1\": {\n","          \"fixed_threshold\": True\n","    }\n","}      \n","\n","import warnings\n","warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA', FutureWarning)\n","warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA', FutureWarning)\n","\n","orion = Orion(\n","    pipeline = [\n","      \"mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences\",\n","      \"numpy.reshape\",\n","      \"statsmodels.tsa.arima_model.Arima\",\n","      \"orion.primitives.timeseries_errors.regression_errors\",\n","      \"orion.primitives.timeseries_anomalies.find_anomalies\"\n","    ] , #'lstm_dynamic_threshold'\n","    hyperparameters = hyperparameters\n",")\n","\n","print(\"============ FIT ============\")\n","orion.fit(np.array(train_scale).reshape(-1,1), index=np.array(range(len(train_scale))).reshape(-1,1))\n","\n","print(\"============ DETECT ============\")\n","# print(orion.detect(testdata))\n","print(orion.detect(test_scale).reshape(-1,1), index=np.array(range(len(test_scale))).reshape(-1,1))\n","scores = orion.evaluate(testdata, ground_truth, fit=True)"]},{"cell_type":"code","source":["from pandas.tseries.offsets import Second\n","from datetime import date, timedelta\n","from orion import Orion\n","from datetime import datetime\n","\n","def make_timestamp(date, data_len) :\n","  # start_date = datetime.strptime('201802210000', '%Y%m%d%H%M')\n","  start_date = datetime.strptime(date, '%Y%m%d%H%M')\n","  timestamp = time.mktime(start_date.timetuple())\n","  dates = []; dates.append(timestamp)\n","\n","  for i in range(data_len-1) :\n","    new_start_date = start_date + timedelta(minutes=10)\n","    timestamp = time.mktime(new_start_date.timetuple())\n","    dates.append(timestamp)\n","    start_date = new_start_date\n","  \n","  return dates\n","\n","dates = make_timestamp('201802210000', len(train_scale))\n","traindata = pd.DataFrame({\"timestamp\":dates, \"value\":train_scale})\n","dates_test = make_timestamp('201802211200', len(test_scale))\n","testdata = pd.DataFrame({\"timestamp\":dates_test, \"value\":test_scale})\n","\n","hyperparameters = {\n","    \"mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n","      \"target_column\": 0,\n","      \"window_size\": 1000,\n","      \"step_size\": 250\n","    },\n","    \"orion.primitives.timeseries_preprocessing.slice_array_by_dims#1\" : {\n","      \"target_index\" : 0,\n","      \"axis\" : 2\n","    },\n","    \"keras.Sequential.LSTMSeq2Seq#1\" : {\n","        \"epochs\" : 30,\n","        \"window_size\" : 1000,\n","        \"input_size\" : [1000,1],\n","        # \"target_size\" : [1000,1]\n","    },\n","    # \"orion.primitives.timeseries_anomalies.find_anomalies#1\" : {\n","    #     \"window_size_portion\" : 0.3,\n","    #     \"fixed_threshold\" : True\n","    # }\n","}      \n","\n","\n","orion = Orion(\n","  pipeline = [\n","    \"mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences\",\n","    \"orion.primitives.timeseries_preprocessing.slice_array_by_dims\",\n","    \"keras.Sequential.LSTMSeq2Seq\",\n","    # \"orion.primitives.timeseries_errors.reconstruction_errors\",\n","    # \"orion.primitives.timeseries_anomalies.find_anomalies\"\n","  ], #'lstm_dynamic_threshold'\n","  hyperparameters = hyperparameters\n",")\n","\n","print(\"============ FIT ============\")\n","orion.fit(np.array(train_scale).reshape(-1,1), index=np.array(range(len(train_scale))).reshape(-1,1))\n","\n","print(\"============ DETECT ============\")\n","# print(orion.detect(testdata))\n","print(orion.detect(test_scale).reshape(-1,1), index=np.array(range(len(test_scale))).reshape(-1,1))"],"metadata":{"id":"U9CvNNjfr6U3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mu6ZW_WGkVFl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gNZQqPKhgVAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubS_Nsgqv5Ke"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyMTPBVnVFgpCLoytBTGoxDa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}